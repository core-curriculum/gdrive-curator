{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output outcomes\n",
    "\n",
    "depends on download_sheets\n",
    "\n",
    "- tables_index.csv\n",
    "- tables(tables/*.csv)\n",
    "- l1.csv, l2.csv, l3.csv, l4.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output table_index and tables for outcomes\n",
    "\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from lib.utils import OUTPUT_OUTCOMES_DIR,OUTPUT_OUTCOMES_TABLE_DIR\n",
    "from lib.apply_condition_to_dataframe  import apply_condition_to_dataframe\n",
    "from lib.outcomes_utils import get_table_index,iter_tables_for_outcome_raw\n",
    "\n",
    "os.makedirs(OUTPUT_OUTCOMES_DIR,exist_ok=True)\n",
    "os.makedirs(OUTPUT_OUTCOMES_TABLE_DIR,exist_ok=True)\n",
    "\n",
    "for table,info in iter_tables_for_outcome_raw():\n",
    "    table = table\\\n",
    "        .loc[:,[*re.split(r\" *, *\",info.列),\"index\",\"UID\"] ]\n",
    "    print(f\"output... {info.id}.csv\")\n",
    "    table.to_csv(f\"{OUTPUT_OUTCOMES_TABLE_DIR}/{info.id}.csv\",index=False)\n",
    "\n",
    "table_index = get_table_index().drop(\"データ元\",axis=1)\n",
    "table_index.to_csv(f\"{OUTPUT_OUTCOMES_DIR}/tables_index.csv\",encoding=\"utf_8_sig\",index=False)\n",
    "print(f\"output... tables_index.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output outcomes l1,l2,l3,l4\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "from lib.utils import BASE_DIR,SHEETS_OUTCOMES_DIR,OUTPUT_OUTCOMES_DIR,get_glob_file\n",
    "from lib.dataframe_to_grouped_numbers import dataframe_to_grouped_numbers\n",
    "from lib.outcomes_utils import get_table_index,format_table_ref\n",
    "\n",
    "os.makedirs(f\"{OUTPUT_OUTCOMES_DIR}\",exist_ok=True)\n",
    "\n",
    "\n",
    "# reading layer 1\n",
    "r4_l1=pd.read_csv(f\"{SHEETS_OUTCOMES_DIR}/第1層/第1層.csv\")\n",
    "r4_l1=r4_l1.rename(columns={\"第1層イニシャル\":\"index\",\"第1層フルスペル\":\"l1_spell\",\"第1層\":\"l1\",\"第1層説明\":\"l1_desc\"})\n",
    "r4_l1=r4_l1.loc[:,[\"index\",\"UID\",\"l1_spell\",\"l1\",\"l1_desc\"]]\n",
    "r4_l1.to_csv(f\"{OUTPUT_OUTCOMES_DIR}/l1.csv\",encoding=\"utf_8_sig\",quoting=csv.QUOTE_NONNUMERIC,index=False)\n",
    "print(\"output... l1.csv\")\n",
    "\n",
    "\n",
    "l1_indexes = r4_l1[\"index\"]\n",
    "# reading layer 2\n",
    "r4_l2 =  pd.DataFrame(data=[],columns=[])\n",
    "for i, row in r4_l1.iterrows():\n",
    "    l1_index = row[\"index\"]\n",
    "    l1_UID = row[\"UID\"]\n",
    "    filename = get_glob_file(f\"{SHEETS_OUTCOMES_DIR}/{l1_index}*/第2層.csv\")\n",
    "    r4_l2_unit =pd.read_csv(filename,encoding=\"utf_8_sig\") \n",
    "    r4_l2_unit=r4_l2_unit.rename(columns={\"第2層\":\"l2\",\"第2層説明\":\"l2_desc\"})\n",
    "    r4_l2_unit[\"index\"] = l1_index+\"-\"+(r4_l2_unit.index+1).astype(\"str\").str.zfill(2)\n",
    "    r4_l2_unit[\"l1_index\"] = l1_index\n",
    "    r4_l2_unit[\"l1_UID\"] = l1_UID\n",
    "    r4_l2_unit = r4_l2_unit.loc[:,[\"index\",\"UID\",\"l2\",\"l2_desc\",\"l1_index\",\"l1_UID\"]]\n",
    "    r4_l2=pd.concat([r4_l2,r4_l2_unit])\n",
    "r4_l2.to_csv(f\"{OUTPUT_OUTCOMES_DIR}/l2.csv\",encoding=\"utf_8_sig\",quoting=csv.QUOTE_NONNUMERIC,index=False)\n",
    "print(\"output... l2.csv\")\n",
    "\n",
    "# reading layer 3 and 4\n",
    "r4_l3 =  pd.DataFrame(data=[],columns=[])\n",
    "r4_l4 =  pd.DataFrame(data=[],columns=[])\n",
    "for l1_index in l1_indexes:\n",
    "    filename = get_glob_file(f\"{SHEETS_OUTCOMES_DIR}/{l1_index}*/第2から4層.csv\")\n",
    "    r4_l234_unit=pd.read_csv(filename)\n",
    "    r4_l234_unit=r4_l234_unit.rename(columns={\"第2層\":\"l2\",\"第3層\":\"l3\",\"第4層\":\"l4\",\"H28対応項目\":\"H28ID\"})\n",
    "    r4_l234_unit = pd.merge(r4_l234_unit,r4_l2.rename(columns={\"UID\":\"l2_UID\",\"index\":\"l2_index\"}),how=\"left\",on=\"l2\")\n",
    "    ids=dataframe_to_grouped_numbers(r4_l234_unit,[\"l2\",\"l3\",\"l4\"])\n",
    "\n",
    "    # reading layer 3\n",
    "    r4_l234_unit[\"l3_index\"]=r4_l234_unit[\"l2_index\"]+\"-\"+ids[\"l3\"].astype(\"str\").str.zfill(2)\n",
    "    r4_l3_unit = r4_l234_unit.loc[:,[\"l3_index\",\"l3_UID\",\"l3\",\"l2_index\",\"l2_UID\"]]\n",
    "    r4_l3_unit = r4_l3_unit.rename(columns={\"l3_index\":\"index\",\"l3_UID\":\"UID\"})\n",
    "    r4_l3 = pd.concat([r4_l3,r4_l3_unit.drop_duplicates(subset=[\"index\"])]) \n",
    "\n",
    "    # reading layer 4\n",
    "    r4_l234_unit[\"l4_index\"]=r4_l234_unit[\"l3_index\"]+\"-\"+ids[\"l4\"].astype(\"str\").str.zfill(2)\n",
    "    r4_l4_unit = r4_l234_unit.loc[:,[\"l4_index\",\"UID\",\"l4\",\"l3_index\",\"l3_UID\",\"H28ID\"]]\n",
    "    r4_l4_unit = r4_l4_unit.rename(columns={\"l4_index\":\"index\"})\n",
    "    r4_l4 = pd.concat([r4_l4,r4_l4_unit]) \n",
    "\n",
    "\n",
    "r4_l4[\"l4\"] = r4_l4[\"l4\"].map(format_table_ref)\n",
    "r4_l3[\"l3\"] = r4_l3[\"l3\"].map(format_table_ref)\n",
    "\n",
    "\n",
    "r4_l3.to_csv(f\"{OUTPUT_OUTCOMES_DIR}/l3.csv\",encoding=\"utf_8_sig\",quoting=csv.QUOTE_NONNUMERIC,index=False)\n",
    "print(\"output... l3.csv\")\n",
    "\n",
    "\n",
    "r4_l4.to_csv(f\"{OUTPUT_OUTCOMES_DIR}/l4.csv\",encoding=\"utf_8_sig\",quoting=csv.QUOTE_NONNUMERIC,index=False)\n",
    "print(\"output... l4.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
